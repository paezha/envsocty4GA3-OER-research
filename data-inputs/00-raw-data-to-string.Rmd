---
title: "00-raw-data-to-string"
output: html_document
date: "2022-11-15"
---
In this notebook we prepare the text for analysis. There are several tasks:

1. Extract the files from the zip folders and rename them so they are anonymous.
2. Clean the PDF files and extract text.
3. Clean the DOCX and RTF files and extract text.
4. Bind the corpus all together and save to data folder. 


```{r}
library(filesstrings) # Handy File and String Manipulation
library(pdftools) # Text Extraction, Rendering and Converting of PDF Documents
library(tidyverse) # Easily Install and Load the 'Tidyverse'
## THIS PACKAGE HAS BEEN DISCONTINUED; IT CAN BE INSTALLED WITH renv::install("textreadr@1.2.0") or obtained from here: https://cran.r-project.org/src/contrib/Archive/textreadr/
library(textreadr) # functions to convert docx and rtf to text
```


# -- Task 1: unzip raw data files and organize the raw-data folder

Unzip data files, some are pdfs, docx. and rtf. They are also different formats; some have title text, some have names+student numbers, some have the question for the assignment. Our goal is to extract the reflection text ONLY from the files - to keep it anonymous and to only include reflection text as part of the analysis:
```{r}
# so here, make sure the raw-data folder only contains "Data-for-Reflection-Analysis.zip" and nothing else. Otherwise this code will have reproducibility issues.

#unzip
unzip("raw-data/Data-for-Reflection-Analysis.zip")
#unlink("Named-Reflections-ENVSOCTY4GA3", recursive=TRUE)

#move files from Named-Reflections-ENVSOCTY4GA3 into the raw-data/ folder
# 2020
file_paths_2020 <- list.files(path = "Data-for-Reflection-Analysis/2020-Data-for-Reflection-Analysis/Named-Reflections-ENVSOCTY4GA3", full.names = TRUE) # here we create a string of each files directory
move_files(files=file_paths_2020, destinations = "raw-data/reflections") #move all files into 'raw-data'
# 2021
file_paths_2021 <- list.files(path = "Data-for-Reflection-Analysis/2021-Data-for-Reflection-Analysis/Named-Reflections-ENVSOCTY4GA3", full.names = TRUE) # here we create a string of each files directory
move_files(files=file_paths_2021, destinations = "raw-data/reflections") #move all files into 'raw-data'
# 2022
file_paths_2022 <- list.files(path = "Data-for-Reflection-Analysis/2022-Data-for-Reflection-Analysis/Named-Reflections-ENVSOCTY4GA3", full.names = TRUE) # here we create a string of each files directory
move_files(files=file_paths_2022, destinations = "raw-data/reflections") #move all files into 'raw-data'
# 2024
file_paths_2024 <- list.files(path = "Data-for-Reflection-Analysis/2024-Data-for-Reflection-Analysis/Named-Reflections-ENVSOCTY4GA3", full.names = TRUE) # here we create a string of each files directory
move_files(files=file_paths_2024, destinations = "raw-data/reflections") #move all files into 'raw-data'

unlink("Data-for-Reflection-Analysis", recursive=TRUE) #remove the, now empty, Named-Reflection-ENVSOCTY4GA3 folder

#Great! Now all the files from ENVSOCTY4GA3 are in the raw-data folder, along with the original 'Data-for-Reflection-Analysis.zip'
```

# -- Task 2: Clean files in raw-data folder such that all files are in PDF file type and only 'reflection' text remains.

Before starting, let's delete the duplicate submission (manually identified) and then rename the files so they have short and anonyomous names
```{r}
#folder location of the reflections
folder <- paste0(getwd(), "/raw-data/reflections")

file_paths <- list.files(path = folder, full.names = TRUE)
file_names <- list.files(path = folder, full.names = FALSE)
#file_exts <- sapply(strsplit(file_names, split = "\\."), `[`, 2) 

#files by year
file_names_2020 <- file_names[grepl("137554", file_paths)] 
file_exts_2020 <- sapply(strsplit(file_names_2020, split = "\\."), `[`, 2) 

file_names_2021 <- file_names[grepl(" 2021 ", file_paths)] 
file_exts_2021 <- sapply(strsplit(file_names_2021, split = "\\."), `[`, 2) 

file_names_2022 <- file_names[grepl(" 2022 ", file_paths)] 
file_exts_2022 <- sapply(strsplit(file_names_2022, split = "\\."), `[`, 2) 

file_names_2024 <- file_names[grepl(" 2024 ", file_paths)] 
file_exts_2024 <- sapply(strsplit(file_names_2024, split = "\\."), `[`, 2) 

ids_2020 <- paste0(folder, "/2020-", 1:length(file_names_2020), ".", file_exts_2020)
ids_2021 <- paste0(folder, "/2021-", 1:length(file_names_2021), ".", file_exts_2021)
ids_2022 <- paste0(folder, "/2022-", 1:length(file_names_2022), ".", file_exts_2022)
ids_2024 <- paste0(folder, "/2024-", 1:length(file_names_2024), ".", file_exts_2024)

#rename files to date them
file.rename(from = paste0(folder, "/", file_names_2020),
            to = ids_2020)
file.rename(from = paste0(folder, "/", file_names_2021),
            to = ids_2021)
file.rename(from = paste0(folder, "/", file_names_2022),
            to = ids_2022)
file.rename(from = paste0(folder, "/", file_names_2024),
            to = ids_2024)


#new file paths
file_paths <- list.files(path = folder, full.names = TRUE)
#new file names
file_names <- list.files(path = folder, full.names = FALSE)

#files by format
file_paths_docx <- file_paths[grepl(".docx", file_paths)]
file_names_docx <- file_names[grepl(".docx", file_names)]
file_paths_rtf <- file_paths[grepl(".rtf", file_paths)]
file_names_rtf <- file_names[grepl(".rtf", file_names)]

#creating a vector of new file names
ids_docx <- paste0(folder, "/DOCX-", file_names_docx)
ids_rtf <- paste0(folder, "/RTF-", file_names_rtf)
```

Rename all files so that they can be identified by format and year:
```{r}
# now we copy all the files but give them new names and remove the duplicated files (that have the old names)
file.rename(from = file_paths_docx, to = ids_docx)
file.rename(from = file_paths_rtf, to = ids_rtf)
```

We're ready to extract text! 

# -- Task 3: Clean files in raw-data folder such that all files are in DOCX and RTF file type and only 'reflection' text remains.

Let's do DOCX now! We will have to use 'read_docx' function from the textreadr package. It uses the 'antiword' engine to extract text from word documents.
```{r}
#make the data frame
corpus_raw <- data.frame(title = NULL,
                         text = NULL)

#cycle for the text fetching: 
for (i in 1:length(ids_docx)){
    # #print i so that I know the loop is working right
    # print(i)
  corpus_raw <- rbind(corpus_raw,
                      data.frame(title = gsub(pattern = "",
                                              replacement = "",
                                              x = file_names_docx[i], ignore.case = T),
                                 text = paste0(read_docx(ids_docx[i]), collapse = "\n")))
}

corpus_raw <- corpus_raw |>
  mutate(text = str_trim(text), # Remove padding blank spaces
         # Replace line break symbols with spaces
         text = str_replace_all(text,
                                "[\r\n]",
                                " "),
         # Remove unnecessary blank spaces in text
         text = str_squish(text))


corpus_raw_docx <- corpus_raw
corpus_raw_docx[2] #viewing the 'text' of the docx This is a check to see if the manual-ish gsub text removals worked correctly
```

Let's do RTF now! We only have 1 RTF file... so no need for a for loop for this one. I'll use 'read_rtf' function from the textreadr package. 
```{r}
#make the data frame
corpus_raw <- data.frame(title = NULL,
                         text = NULL)

#cycle for the text fetching: 
for (i in 1:length(ids_rtf)){
    # #print i so that I know the loop is working right
    # print(i)
  corpus_raw <- rbind(corpus_raw,
                      data.frame(title = gsub(pattern = "",
                                              replacement = "",
                                              x = file_names_rtf[i], ignore.case = T),
                                 text = paste0(read_rtf(ids_rtf[i]), collapse = "\n")))
}

corpus_raw <- corpus_raw |>
  mutate(text = str_trim(text), # Remove padding blank spaces
         # Replace line break symbols with spaces
         text = str_replace_all(text,
                                "[\r\n]",
                                " "),
         # Remove unnecessary blank spaces in text
         text = str_squish(text))

corpus_raw_rtf <- corpus_raw
corpus_raw_rtf[2] #viewing the 'text' of the docx This is a check to see if the manual-ish gsub text removals worked correctly
```

# -- Task 4: Clean the repository.

Delete the folder with all the reflections:
```{r}
unlink("raw-data/reflections", recursive=TRUE) #remove the, now empty, Named-Reflection-ENVSOCTY4GA3 folder
```

# -- Task 5: Merge all text reflection data frames into one anonymous frame and save it into 'data'.

Merge all data frames
```{r}
corpus_raw <- rbind(corpus_raw_docx, corpus_raw_rtf)
```

Save data:
```{r}
dir.create(paste0(here::here(), "/data"))

save(corpus_raw, 
     file = paste0(here::here(), "/data/corpus_raw.rda"),
     compress = "xz")
```


